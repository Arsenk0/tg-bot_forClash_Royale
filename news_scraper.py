import requests
from bs4 import BeautifulSoup
import json
import os
import re

NEWS_HISTORY_FILE = "news_history.json"


def clean_image_url(url):
    """
    Ð’Ð¸Ð´Ð°Ð»ÑÑ” Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¸ ÑÑ‚Ð¸ÑÐ½ÐµÐ½Ð½Ñ (cdn-cgi), Ñ‰Ð¾Ð± Ð¾Ñ‚Ñ€Ð¸Ð¼Ð°Ñ‚Ð¸ Ñ‡Ð¸ÑÑ‚Ð¸Ð¹ Ð¾Ñ€Ð¸Ð³Ñ–Ð½Ð°Ð».
    """
    if not url: return None
    # Ð’Ð¸Ð´Ð°Ð»ÑÑ”Ð¼Ð¾ Ñ‡Ð°ÑÑ‚Ð¸Ð½Ñƒ cdn-cgi/image/.../ Ñ‰Ð¾Ð± ÑÐµÑ€Ð²ÐµÑ€ Ð²Ñ–Ð´Ð´Ð°Ð² Ð¾Ñ€Ð¸Ð³Ñ–Ð½Ð°Ð»
    clean = re.sub(r'cdn-cgi/image/[^/]+/', '', url)
    return clean


def get_best_image_source(img_tag):
    """
    ÐœÐ°Ð³Ñ–Ñ Ð·Ñ– ÑÐºÑ€Ñ–Ð½ÑˆÐ¾Ñ‚Ñƒ:
    RoyaleAPI Ñ…Ð¾Ð²Ð°Ñ” HD ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÑƒ Ð² Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ‚Ð¸ 'data-zoom-src' Ð°Ð±Ð¾ 'data-src'.
    Ð—Ð²Ð¸Ñ‡Ð°Ð¹Ð½Ð¸Ð¹ 'src' Ñ‚Ð°Ð¼ Ñ‡Ð°ÑÑ‚Ð¾ ÑÑ‚Ð¸ÑÐ½ÐµÐ½Ð¸Ð¹.
    """
    # 1. ÐÐ°Ð¹ÐºÑ€Ð°Ñ‰Ð¸Ð¹ Ð²Ð°Ñ€Ñ–Ð°Ð½Ñ‚ (Zoom Ð²ÐµÑ€ÑÑ–Ñ, ÑÐº Ð½Ð° ÑÐºÑ€Ñ–Ð½ÑˆÐ¾Ñ‚Ñ–)
    if img_tag.get('data-zoom-src'):
        return img_tag.get('data-zoom-src')

    # 2. Ð”ÑƒÐ¶Ðµ Ñ…Ð¾Ñ€Ð¾ÑˆÐ¸Ð¹ Ð²Ð°Ñ€Ñ–Ð°Ð½Ñ‚ (Lazy Load Ð¾Ñ€Ð¸Ð³Ñ–Ð½Ð°Ð»)
    if img_tag.get('data-src'):
        return img_tag.get('data-src')

    # 3. Ð—Ð²Ð¸Ñ‡Ð°Ð¹Ð½Ð¸Ð¹ Ð²Ð°Ñ€Ñ–Ð°Ð½Ñ‚ (ÑÐºÑ‰Ð¾ Ñ–Ð½ÑˆÐ¸Ñ… Ð½ÐµÐ¼Ð°Ñ”)
    return img_tag.get('src')


def fetch_blog_infographic(article_url, promo_filename_part):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        response = requests.get(article_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")

        # 1. Ð¨ÑƒÐºÐ°Ñ”Ð¼Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€ id="blog_content" (ÑÐº Ð½Ð° Ñ‚Ð²Ð¾Ñ”Ð¼Ñƒ ÑÐºÑ€Ñ–Ð½ÑˆÐ¾Ñ‚Ñ–)
        content_div = soup.find(id="blog_content")

        # Ð¯ÐºÑ‰Ð¾ Ñ€Ð°Ð¿Ñ‚Ð¾Ð¼ id Ð·Ð¼Ñ–Ð½ÑÑ‚ÑŒ, ÑˆÑƒÐºÐ°Ñ”Ð¼Ð¾ Ð¿Ð¾ ÐºÐ»Ð°ÑÑƒ
        if not content_div:
            content_div = soup.find("div", class_="ui segment")

        if not content_div: return None

        # 2. Ð¨ÑƒÐºÐ°Ñ”Ð¼Ð¾ Ð²ÑÑ– ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ¸ Ð²ÑÐµÑ€ÐµÐ´Ð¸Ð½Ñ– Ñ‚ÐµÐºÑÑ‚Ñƒ
        images = content_div.find_all("img")

        candidates = []

        for img in images:
            raw_url = get_best_image_source(img)
            if not raw_url: continue

            # Ð Ð¾Ð±Ð¸Ð¼Ð¾ Ð¿Ð¾ÑÐ¸Ð»Ð°Ð½Ð½Ñ Ð°Ð±ÑÐ¾Ð»ÑŽÑ‚Ð½Ð¸Ð¼
            if raw_url.startswith("/"):
                raw_url = "https://royaleapi.com" + raw_url

            clean_url = clean_image_url(raw_url)
            lower_url = clean_url.lower()

            # --- Ð¤Ð†Ð›Ð¬Ð¢Ð Ð˜ ---

            # Ð. Ð†Ð³Ð½Ð¾Ñ€ÑƒÑ”Ð¼Ð¾ ÑÐ¼Ñ–Ñ‚Ñ‚Ñ
            if any(x in lower_url for x in ["icon", "avatar", "logo", "badge", "social", "pixel"]):
                continue

            # Ð‘. Ð†Ð³Ð½Ð¾Ñ€ÑƒÑ”Ð¼Ð¾ ÐŸÐ ÐžÐœÐž (Ð¾Ð±ÐºÐ»Ð°Ð´Ð¸Ð½ÐºÑƒ)
            # ÐœÐ¸ Ð¿ÐµÑ€ÐµÐ²Ñ–Ñ€ÑÑ”Ð¼Ð¾, Ñ‡Ð¸ Ñ” Ð² Ð½Ð°Ð·Ð²Ñ– Ñ„Ð°Ð¹Ð»Ñƒ ÑÐ»Ð¾Ð²Ð¾ "promo"
            if "promo" in lower_url:
                continue

            # Ð’. Ð”Ð¾Ð´Ð°Ñ‚ÐºÐ¾Ð²Ð° Ð¿ÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ°: ÑÐºÑ‰Ð¾ Ð¼Ð¸ Ð·Ð½Ð°Ñ”Ð¼Ð¾ Ð½Ð°Ð·Ð²Ñƒ Ð¾Ð±ÐºÐ»Ð°Ð´Ð¸Ð½ÐºÐ¸, Ñ–Ð³Ð½Ð¾Ñ€ÑƒÑ”Ð¼Ð¾ Ñ—Ñ— Ñ‚Ð¾Ñ‡Ð½Ñƒ ÐºÐ¾Ð¿Ñ–ÑŽ
            if promo_filename_part and promo_filename_part in lower_url:
                continue

            # Ð¯ÐºÑ‰Ð¾ Ð¿Ñ€Ð¾Ð¹ÑˆÐ»Ð¸ Ð²ÑÑ– Ñ„Ñ–Ð»ÑŒÑ‚Ñ€Ð¸ â€” Ñ†Ðµ Ð²Ð¾Ð½Ð¾!
            candidates.append(clean_url)

        # Ð‘ÐµÑ€ÐµÐ¼Ð¾ Ð¿ÐµÑ€ÑˆÑƒ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ñƒ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÑƒ Ð· Ñ‚ÐµÐºÑÑ‚Ñƒ, ÑÐºÐ° Ð½Ðµ Ñ” Ð¿Ñ€Ð¾Ð¼Ð¾
        if candidates:
            return candidates[0]

    except Exception as e:
        print(f"ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð¿Ð¾ÑˆÑƒÐºÑƒ Ñ–Ð½Ñ„Ð¾Ð³Ñ€Ð°Ñ„Ñ–ÐºÐ¸: {e}")
        return None

    return None


def get_latest_news():
    url = "https://royaleapi.com/blog"
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")

        posts = soup.find_all("div", class_="segment")

        for post in posts:
            header = post.find("h2") or post.find("h3")
            if not header: continue

            title = header.get_text().strip()
            link_tag = post.find("a")
            if not link_tag: continue

            link = "https://royaleapi.com" + link_tag['href']

            # --- ÐžÐ¢Ð Ð˜ÐœÐ£Ð„ÐœÐž Ð”ÐÐÐ† ÐŸÐ Ðž ÐžÐ‘ÐšÐ›ÐÐ”Ð˜ÐÐšÐ£ ---
            # Ð©Ð¾Ð± Ð·Ð½Ð°Ñ‚Ð¸, Ñ‰Ð¾ Ñ–Ð³Ð½Ð¾Ñ€ÑƒÐ²Ð°Ñ‚Ð¸ Ð²ÑÐµÑ€ÐµÐ´Ð¸Ð½Ñ–
            img_tag = post.find("img")
            promo_img = img_tag['src'] if img_tag else ""

            # Ð’Ð¸Ñ‚ÑÐ³ÑƒÑ”Ð¼Ð¾ ÐºÐ»ÑŽÑ‡Ð¾Ð²Ñƒ Ñ‡Ð°ÑÑ‚Ð¸Ð½Ñƒ Ð½Ð°Ð·Ð²Ð¸ Ñ„Ð°Ð¹Ð»Ñƒ Ð¿Ñ€Ð¾Ð¼Ð¾ (Ð½Ð°Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´ "s79-balance-promo")
            # Ð©Ð¾Ð± Ð²Ð¸Ð¿Ð°Ð´ÐºÐ¾Ð²Ð¾ Ð½Ðµ Ð²Ð·ÑÑ‚Ð¸ Ñ—Ñ— Ð¶ Ð·ÑÐµÑ€ÐµÐ´Ð¸Ð½Ð¸ ÑÑ‚Ð°Ñ‚Ñ‚Ñ–
            promo_filename_part = ""
            if promo_img:
                promo_filename_part = promo_img.split("/")[-1].replace(".jpg", "").replace(".png", "")

            print(f"ðŸ”Ž ÐÐ¾Ð²Ð¸Ð½Ð°: {title}")

            # --- Ð¨Ð£ÐšÐÐ„ÐœÐž Ð¡ÐŸÐ ÐÐ’Ð–ÐÐ® Ð†ÐÐ¤ÐžÐ“Ð ÐÐ¤Ð†ÐšÐ£ ---
            final_image = fetch_blog_infographic(link, promo_filename_part)

            if final_image:
                print(f"   âœ… Ð—Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ñ–Ð½Ñ„Ð¾Ð³Ñ€Ð°Ñ„Ñ–ÐºÑƒ: {final_image}")
            else:
                print("   âš ï¸ Ð†Ð½Ñ„Ð¾Ð³Ñ€Ð°Ñ„Ñ–ÐºÐ¸ Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾, Ð±ÐµÑ€ÐµÐ¼Ð¾ Ð¿Ñ€Ð¾Ð¼Ð¾ (HD).")
                # Ð¯ÐºÑ‰Ð¾ Ð²Ð¶Ðµ Ð·Ð¾Ð²ÑÑ–Ð¼ Ð½Ñ–Ñ‡Ð¾Ð³Ð¾ Ð½ÐµÐ¼Ð°Ñ”, Ð±ÐµÑ€ÐµÐ¼Ð¾ Ð¾Ð±ÐºÐ»Ð°Ð´Ð¸Ð½ÐºÑƒ, Ð°Ð»Ðµ Ð² HD
                final_image = clean_image_url(
                    "https://royaleapi.com" + promo_img if promo_img.startswith("/") else promo_img)

            # Ð—Ð±ÐµÑ€Ñ–Ð³Ð°Ñ”Ð¼Ð¾ ID (Ñ€Ð¾Ð·ÐºÐ¾Ð¼ÐµÐ½Ñ‚ÑƒÐ¹ Ð´Ð»Ñ Ñ€Ð¾Ð±Ð¾Ñ‚Ð¸)
            save_news_id(link)

            return {
                "title": title,
                "link": link,
                "image": final_image
            }

    except Exception as e:
        print(f"ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð³Ð¾Ð»Ð¾Ð²Ð½Ð¾Ð³Ð¾ Ð¿Ð°Ñ€ÑÐµÑ€Ð°: {e}")
        return None


def is_news_old(news_id):
    if not os.path.exists(NEWS_HISTORY_FILE): return False
    try:
        with open(NEWS_HISTORY_FILE, "r") as f:
            history = json.load(f)
        return news_id in history
    except:
        return False


def save_news_id(news_id):
    history = []
    if os.path.exists(NEWS_HISTORY_FILE):
        with open(NEWS_HISTORY_FILE, "r") as f:
            history = json.load(f)

    if news_id not in history:
        history.append(news_id)

    if len(history) > 20: history = history[-20:]

    with open(NEWS_HISTORY_FILE, "w") as f:
        json.dump(history, f)